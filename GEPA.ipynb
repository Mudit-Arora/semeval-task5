{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "bLQWR4XDZFFL",
        "outputId": "630cb492-34b4-4686-d419-eed1e26852c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'ollama'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1022217380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mollama\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAsyncClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mField\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpydantic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_construction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelMetaclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ollama'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from ollama import AsyncClient\n",
        "from pydantic import BaseModel, Field\n",
        "from pydantic._internal._model_construction import ModelMetaclass\n",
        "import ast\n",
        "import random\n",
        "from difflib import SequenceMatcher\n",
        "from colored import Fore, Style, Back\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import sys\n",
        "import time\n",
        "import threading\n",
        "import asyncio\n",
        "from typing import Any, Union\n",
        "import time\n",
        "import yaml\n",
        "import json\n",
        "import textwrap\n",
        "from google import genai\n",
        "\n",
        "\n",
        "#GLOBAL VARIABLES\n",
        "STOP_SPINNER = False\n",
        "\n",
        "with open(\"GEPA_cfg.yaml\", \"r\") as f:\n",
        "    cfg = yaml.load(f, Loader=yaml.SafeLoader)\n",
        "\n",
        "TARGET_MODEL = cfg.get('TARGET_MODEL', 'gemma3n:e4b')\n",
        "REFLECTION_MODEL = cfg.get('REFLECTION_MODEL', 'gemma3n:e4b')\n",
        "USE_GEMINI_API = cfg.get('USE_GEMINI_API', False)\n",
        "\n",
        "if USE_GEMINI_API:\n",
        "    VERTEXAI = cfg.get('VERTEXAI', False)\n",
        "    PROJECT = cfg.get('PROJECT', '')\n",
        "    LOCATION = cfg.get('LOCATION', '')\n",
        "\n",
        "    if (not VERTEXAI) or (not PROJECT) or (not LOCATION):\n",
        "        raise ValueError(\"Missing required configuration for Gemini API.\")\n",
        "\n",
        "P = cfg.get('EXPLOIT_PROB', 0.95)\n",
        "Q = cfg.get('MERGE_PROB', 0.90)\n",
        "BUDGET = cfg.get('BUDGET', 50)\n",
        "MINI_BATCH_SIZE = cfg.get('MINI_BATCH_SIZE', 2)\n",
        "NUM_INITIAL_CANDIDATE_PROMPTS = cfg.get('NUM_INITIAL_CANDIDATE_PROMPTS', 6)\n",
        "\n",
        "#PROMPTS\n",
        "SEED_PROMPT = \"You are an assistant and your task is to answer the user's question.\"\n",
        "\n",
        "META_PROMPT = (\"[[ ## context ## ]]\\n\"\n",
        "               \"You are the reflection model, an expert at improving large language model prompts for a specific task. \"\n",
        "               \"You will be given:\\n\"\n",
        "               \"1. The current prompt (used by the target model for the task).\\n\"\n",
        "               \"2. A set of feedback notes summarizing where the system underperformed.\\n\\n\"\n",
        "               \"[[ ## current prompt ## ]]\\n\"\n",
        "               \"{candidate}\\n\\n\"\n",
        "               \"The following are examples of different task inputs provided to the assistant along \"\n",
        "               \"with the assistant's response for each of them, and feedback on how the assistant's \"\n",
        "               \"response could be better:\\n\"\n",
        "               \"[[ ## inputs, outputs, feedback ## ]]\\n\"\n",
        "               \"{inputs_outputs_feedback}\\n\\n\"\n",
        "               \"[[ ## goal ## ]]\\n\"\n",
        "               \"Propose at most 1 new prompt that keeps what works, fixes weaknesses, improves clarity, \"\n",
        "               \"completeness, and correctness, and is concise. Output ONLY the new improved prompt text. \"\n",
        "               \"Do NOT generate examples from the task.\")\n",
        "\n",
        "INITIAL_CANDIDATES_GENERATION_PROMPT = (\"[[ ## context ## ]]\\n\"\n",
        "                                        \"You are an expert at generating potentially optimal large language model prompts or instructions \"\n",
        "                                        \"for a specific task. You will be given a seed prompt and sampled items from a dataset related to \"\n",
        "                                        \"the task to help you diagnose what the task is about.\\n\"\n",
        "                                        \"[[ ## seed prompt ## ]]\\n\"\n",
        "                                        \"{seed_prompt}\\n\\n\"\n",
        "                                        \"[[ ## inputs, outputs ## ]]\\n\"\n",
        "                                        \"{inputs}\\n\\n\"\n",
        "                                        \"[[ ## goal ## ]]\\n\"\n",
        "                                        \"Generate a set of diverse and high-quality candidate prompts or instructions for the task \"\n",
        "                                        \"using the seed prompt as your reference. Provide at most {num_new_prompts} new candidate prompts. \"\n",
        "                                        \"Output ONLY the synthesized candidate prompts. \"\n",
        "                                        \"Do NOT generate examples from the task.\")\n",
        "\n",
        "MERGING_PROMPT = (\"[[ ## context ## ]]\\n\"\n",
        "                  \"You are an expert at merging text prompts. You will be given candidate prompts to synthesize:\\n\"\n",
        "                  \"[[ ## candidate prompts ## ]]\\n\"\n",
        "                  \"{candidates}\\n\\n\"\n",
        "                  \"[[ ## goal ## ]]\\n\"\n",
        "                  \"Synthesize the candidate prompts into a single, coherent prompt that captures the essence of all the candidates. \"\n",
        "                  \"The synthesis must be done such that the weaknesses of the individual candidates are addressed. Output ONLY the \"\n",
        "                  \"synthesized candidate prompts. Output ONLY the merged candidate prompt. Do NOT generate examples from the task.\")\n",
        "\n",
        "#BASEMODELS/MODELMETACLASSES\n",
        "class CandidatePrompt(BaseModel):\n",
        "    candidate_prompt: str = Field(..., description=\"The candidate prompt to be evaluated.\")\n",
        "\n",
        "class ListCandidatePrompts(BaseModel):\n",
        "    list_candidate_prompts: list[CandidatePrompt] = Field(..., description=\"A list of candidate prompts generated by the reflection model.\")\n",
        "\n",
        "class Response(BaseModel):\n",
        "    response: str = Field(..., description=\"The final response provided by the assistant.\")\n",
        "\n",
        "\n",
        "def rich_display(text: str, style: str = \"info\") -> str:\n",
        "    styles = {\n",
        "        \"info\": f\"{Fore.CYAN}â„¹ï¸ \",\n",
        "        \"success\": f\"{Fore.GREEN}âœ…\",\n",
        "        \"warning\": f\"{Fore.YELLOW}âš ï¸ \",\n",
        "        \"error\": f\"{Fore.RED}âŒ\",\n",
        "        \"scoring\": f\"{Fore.MAGENTA}ðŸ“Š\",\n",
        "        \"comparing\": f\"{Fore.BLUE}ðŸ”\",\n",
        "        \"evaluating\": f\"{Fore.VIOLET}ðŸ§ª\",\n",
        "        \"rollout\": f\"{Back.CYAN}ðŸ”„\",\n",
        "    }\n",
        "    print(f\"{Style.BOLD}{styles.get(style, Fore.WHITE)} {text}{Style.reset}\")\n",
        "\n",
        "def eval_metric(pred: str, gt: str) -> float:\n",
        "    \"\"\"This function measures the exactness or similarity between the assistant's response and the ground truth.\"\"\"\n",
        "    ratio = round(SequenceMatcher(None, pred, gt).ratio(), 3)\n",
        "    return ratio\n",
        "\n",
        "def eval_feedback(ratio: float) -> str:\n",
        "    \"\"\"This function returns feedback based on the measured evaluation metric.\"\"\"\n",
        "    if ratio == 1:\n",
        "        return \"Correct answer.\"\n",
        "    else:\n",
        "        return (\"Incorrect answer. The assistant's response may be any or combination of the following:\\n\"\n",
        "                \"- Overly verbose or lengthy explanations\\n\"\n",
        "                \"- Inconsistent with the ground truth's expected format (e.g., units not included, rounding up or down, decimal places not matching)\\n\"\n",
        "                \"- Miscalculations\\n\"\n",
        "                \"- Misinterpretations\")\n",
        "\n",
        "async def generate_content(model: str, prompt: str, format: BaseModel) -> str:\n",
        "    if isinstance(format, BaseModel) or isinstance(format, ModelMetaclass):\n",
        "        format = format.model_json_schema()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid format type. Format must be a Pydantic BaseModel/ModelMetaclass.\")\n",
        "\n",
        "    response = await AsyncClient().generate(\n",
        "        model=model,\n",
        "        prompt=prompt,\n",
        "        format=format)\n",
        "\n",
        "    return response\n",
        "\n",
        "if not USE_GEMINI_API:\n",
        "    async def extract_response_from_target(contents: str, format: Union[BaseModel, ModelMetaclass]) -> str:\n",
        "        global TARGET_MODEL\n",
        "        response = await generate_content(model=TARGET_MODEL, prompt=contents, format=format)\n",
        "\n",
        "        parsed_response_str = response.response or \"\"\n",
        "        # Clean common wrappers / fences and stray indentation\n",
        "        parsed_response_str = parsed_response_str.strip().strip(\"\\t\")\n",
        "        if parsed_response_str.startswith(\"```\"):\n",
        "            # Remove possible code fences like ```json ... ```\n",
        "            lines = parsed_response_str.splitlines()\n",
        "            if lines and lines[0].startswith(\"```\"):\n",
        "                lines = lines[1:]\n",
        "            if lines and lines[-1].startswith(\"```\"):\n",
        "                lines = lines[:-1]\n",
        "            parsed_response_str = \"\\n\".join(lines).strip()\n",
        "\n",
        "        # Try JSON first, then literal_eval; handle indentation/syntax issues\n",
        "        data = None\n",
        "        try:\n",
        "            data = json.loads(parsed_response_str)\n",
        "        except json.JSONDecodeError:\n",
        "            try:\n",
        "                data = ast.literal_eval(parsed_response_str)\n",
        "            except (ValueError, SyntaxError, IndentationError):\n",
        "                rich_display(\"Failed to parse target model response; returning raw string.\", \"warning\")\n",
        "                return parsed_response_str.strip()\n",
        "\n",
        "        if isinstance(data, dict):\n",
        "            value = data.get('response', '')\n",
        "            if isinstance(value, str):\n",
        "                return value.strip()\n",
        "        return \"\"\n",
        "\n",
        "    async def extract_response_from_reflection(contents: str, format: Union[BaseModel, ModelMetaclass]) -> list[str]:\n",
        "        global REFLECTION_MODEL\n",
        "        response = await generate_content(model=REFLECTION_MODEL, prompt=contents, format=format)\n",
        "\n",
        "        parsed_response_str = (response.response or \"\").strip()\n",
        "\n",
        "        # Remove markdown code fences if present\n",
        "        if parsed_response_str.startswith(\"```\"):\n",
        "            lines = parsed_response_str.splitlines()\n",
        "            # drop first fence line\n",
        "            lines = lines[1:]\n",
        "            # drop last fence line if it's a fence\n",
        "            if lines and lines[-1].strip().startswith(\"```\"):\n",
        "                lines = lines[:-1]\n",
        "            parsed_response_str = \"\\n\".join(lines).strip()\n",
        "\n",
        "        # Extra cleanup: remove leading tabs/spaces uniformly\n",
        "        parsed_response_str = textwrap.dedent(parsed_response_str).strip()\n",
        "\n",
        "        data = None\n",
        "        # Try JSON first\n",
        "        if parsed_response_str:\n",
        "            try:\n",
        "                data = json.loads(parsed_response_str)\n",
        "            except json.JSONDecodeError:\n",
        "                try:\n",
        "                    # Fallback to ast.literal_eval; catch indentation issues and retry after another dedent\n",
        "                    try:\n",
        "                        data = ast.literal_eval(parsed_response_str)\n",
        "                    except IndentationError:\n",
        "                        cleaned = textwrap.dedent(parsed_response_str)\n",
        "                        data = ast.literal_eval(cleaned)\n",
        "                except (SyntaxError, ValueError, IndentationError):\n",
        "                    rich_display(\"Reflection model response parsing failed; returning empty list.\", \"warning\")\n",
        "                    return []\n",
        "        else:\n",
        "            return []\n",
        "\n",
        "        if isinstance(data, dict):\n",
        "            parsed_list = data.get('list_candidate_prompts', [])\n",
        "            if isinstance(parsed_list, list):\n",
        "                return [item.get('candidate_prompt', '').strip()\n",
        "                        for item in parsed_list\n",
        "                        if isinstance(item, dict) and item.get('candidate_prompt', '') and item.get('candidate_prompt', '').strip()]\n",
        "        return []\n",
        "\n",
        "    async def extract_merged_prompt(\n",
        "        for_merging_candidates: str,\n",
        "        merging_prompt: str,\n",
        "        format: Union[BaseModel, ModelMetaclass]\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate a merged prompt and robustly parse the reflection model output.\n",
        "        Handles:\n",
        "        - Stripping code fences\n",
        "        - Dedenting / removing tabs & stray spaces\n",
        "        - JSON parsing first\n",
        "        - Fallback to ast.literal_eval with graceful failure\n",
        "        Returns an empty string if parsing fails or no candidate is found.\n",
        "        \"\"\"\n",
        "        global REFLECTION_MODEL\n",
        "        contents = merging_prompt.format(candidates=for_merging_candidates)\n",
        "        response = await generate_content(model=REFLECTION_MODEL, prompt=contents, format=format)\n",
        "\n",
        "        raw = (getattr(response, \"response\", \"\") or \"\").strip()\n",
        "\n",
        "        # Remove markdown code fences if present\n",
        "        if raw.startswith(\"```\"):\n",
        "            lines = raw.splitlines()\n",
        "            # drop first fence\n",
        "            lines = lines[1:]\n",
        "            # drop last fence if it's a fence\n",
        "            if lines and lines[-1].strip().startswith(\"```\"):\n",
        "                lines = lines[:-1]\n",
        "            raw = \"\\n\".join(lines)\n",
        "\n",
        "        # Normalize whitespace / tabs\n",
        "        raw = raw.replace(\"\\r\", \"\")\n",
        "        raw = raw.strip(\"\\t \").strip()\n",
        "        raw = textwrap.dedent(raw).strip()\n",
        "\n",
        "        data = None\n",
        "        if not raw:\n",
        "            rich_display(\"Merged prompt response empty.\", \"warning\")\n",
        "            return \"\"\n",
        "\n",
        "        # Try JSON first\n",
        "        try:\n",
        "            data = json.loads(raw)\n",
        "        except json.JSONDecodeError:\n",
        "            # Fallback to literal_eval with extra dedent if needed\n",
        "            try:\n",
        "                try:\n",
        "                    data = ast.literal_eval(raw)\n",
        "                except IndentationError:\n",
        "                    cleaned = textwrap.dedent(raw)\n",
        "                    data = ast.literal_eval(cleaned)\n",
        "            except (SyntaxError, ValueError, IndentationError):\n",
        "                rich_display(\"Failed to parse merged prompt response; returning empty string.\", \"warning\")\n",
        "                return \"\"\n",
        "\n",
        "        # Expect structure like {'list_candidate_prompts': [ {'candidate_prompt': '...'} ]}\n",
        "        merged_prompt_list = []\n",
        "        if isinstance(data, dict):\n",
        "            maybe_list = data.get('list_candidate_prompts', [])\n",
        "            if isinstance(maybe_list, list):\n",
        "                merged_prompt_list = [\n",
        "                    (item.get('candidate_prompt', '') if isinstance(item, dict) else '').strip()\n",
        "                    for item in maybe_list\n",
        "                ]\n",
        "                merged_prompt_list = [p for p in merged_prompt_list if p]\n",
        "        else:\n",
        "            rich_display(\"Parsed merged prompt response is not a dict; returning empty string.\", \"warning\")\n",
        "            return \"\"\n",
        "\n",
        "        return merged_prompt_list[0] if merged_prompt_list else \"\"\n",
        "else:\n",
        "    async def extract_response_from_target(contents: str, format: Union[BaseModel, ModelMetaclass]) -> str:\n",
        "        global TARGET_MODEL, VERTEXAI, PROJECT, LOCATION\n",
        "        client = genai.Client(vertexai=VERTEXAI, project=PROJECT, location=LOCATION)\n",
        "        response = client.models.generate_content(\n",
        "            model=TARGET_MODEL,\n",
        "            contents=contents,\n",
        "            config={\n",
        "                \"response_mime_type\": \"application/json\",\n",
        "                \"response_schema\": format\n",
        "            }\n",
        "        )\n",
        "        parsed_response = response.parsed.response\n",
        "        return parsed_response.strip() if parsed_response else \"\"\n",
        "\n",
        "    async def extract_response_from_reflection(contents: str, format: Union[BaseModel, ModelMetaclass]) -> list[str]:\n",
        "        global REFLECTION_MODEL, VERTEXAI, PROJECT, LOCATION\n",
        "        client = genai.Client(vertexai=VERTEXAI, project=PROJECT, location=LOCATION)\n",
        "        response = client.models.generate_content(\n",
        "            model=REFLECTION_MODEL,\n",
        "            contents=contents,\n",
        "            config={\n",
        "                \"response_mime_type\": \"application/json\",\n",
        "                \"response_schema\": format\n",
        "            }\n",
        "        )\n",
        "        parsed_response = response.parsed.list_candidate_prompts\n",
        "        return [item.candidate_prompt.strip() for item in parsed_response if item.candidate_prompt.strip()]\n",
        "\n",
        "    async def extract_merged_prompt(\n",
        "        for_merging_candidates: str,\n",
        "        merging_prompt: str,\n",
        "        format: Union[BaseModel, ModelMetaclass]\n",
        "    ) -> str:\n",
        "        global REFLECTION_MODEL, VERTEXAI, PROJECT, LOCATION\n",
        "        contents = merging_prompt.format(candidates=for_merging_candidates)\n",
        "        client = genai.Client(vertexai=VERTEXAI, project=PROJECT, location=LOCATION)\n",
        "        response = client.models.generate_content(\n",
        "            model=REFLECTION_MODEL,\n",
        "            contents=contents,\n",
        "            config={\n",
        "                \"response_mime_type\": \"application/json\",\n",
        "                \"response_schema\": format\n",
        "            }\n",
        "        )\n",
        "\n",
        "        parsed_response = response.parsed.list_candidate_prompts\n",
        "        return parsed_response[0].candidate_prompt.strip() if parsed_response else \"\"\n",
        "\n",
        "async def run_with_spinner(task: callable, message: str = \"Processing...\") -> None:\n",
        "    global STOP_SPINNER\n",
        "    STOP_SPINNER = False\n",
        "\n",
        "    def spinning_cursor():\n",
        "        global STOP_SPINNER\n",
        "        while not STOP_SPINNER:\n",
        "            for cursor in '|/-\\\\':\n",
        "                sys.stdout.write(f\"{message} {cursor}\\r\")\n",
        "                sys.stdout.flush()\n",
        "                time.sleep(0.1)\n",
        "        sys.stdout.write(' ' * (len(message) + 5) + '\\r')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    spinner_thread = threading.Thread(target=spinning_cursor)\n",
        "    spinner_thread.start()\n",
        "\n",
        "    result = None\n",
        "    try:\n",
        "        result = await task()\n",
        "    finally:\n",
        "        STOP_SPINNER = True\n",
        "        spinner_thread.join()\n",
        "\n",
        "    return result\n",
        "\n",
        "async def generate_initial_candidates_from_seed(\n",
        "    seed_prompt: str,\n",
        "    initial_candidates_generation_prompt: str,\n",
        "    Dpareto: list[dict[str, str]],\n",
        "    num_new_prompts: int = 3,\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Concurrently generate initial candidate prompts from a seed prompt.\n",
        "    Instead of one request returning many, we launch num_new_prompts parallel\n",
        "    single-prompt generations (each asking for at most 1 prompt) to leverage concurrency\n",
        "    and improve diversity. Order is preserved (by task index) and duplicates removed.\n",
        "    \"\"\"\n",
        "    rich_display(f\"Seed prompt >> {seed_prompt}\", \"evaluating\")\n",
        "    rich_display(\"Generating initial candidates from seed prompt...\", \"rollout\")\n",
        "\n",
        "    # Build inputs string (sync, cheap)\n",
        "    inputs_parts = []\n",
        "    for sample in Dpareto:\n",
        "        xi, gt = sample.get(\"question\", \"\"), sample.get(\"answer\", \"\")\n",
        "        if not xi or not gt:\n",
        "            rich_display(\"Sample is missing question or answer.\", \"error\")\n",
        "            raise ValueError(\"Sample is missing question or answer.\")\n",
        "        inputs_parts.append(f\"Query: {xi}\\nAnswer: {gt}\\n\")\n",
        "    inputs = \"\\n\".join(inputs_parts) + \"\\n\"\n",
        "\n",
        "    # Optional: throttle concurrent generations if needed (e.g., model rate limits)\n",
        "    semaphore = asyncio.Semaphore(min(num_new_prompts, 5))\n",
        "\n",
        "    async def fetch_one() -> str:\n",
        "        async with semaphore:\n",
        "            # Ask for only 1 prompt this call\n",
        "            contents = initial_candidates_generation_prompt.format(\n",
        "                seed_prompt=seed_prompt,\n",
        "                inputs=inputs,\n",
        "                num_new_prompts=1\n",
        "            )\n",
        "            resp = await extract_response_from_reflection(\n",
        "                contents=contents,\n",
        "                format=ListCandidatePrompts\n",
        "            )\n",
        "            # resp is a list (0 or 1 element). Return first if present.\n",
        "            return resp[0] if resp else \"\"\n",
        "\n",
        "    async def gather_all():\n",
        "        tasks = [asyncio.create_task(fetch_one()) for _ in range(num_new_prompts)]\n",
        "        results = await asyncio.gather(*tasks, return_exceptions=False)\n",
        "        # Deduplicate while preserving order\n",
        "        seen = set()\n",
        "        unique = []\n",
        "        for r in results:\n",
        "            r_stripped = r.strip()\n",
        "            if r_stripped and r_stripped not in seen:\n",
        "                seen.add(r_stripped)\n",
        "                unique.append(r_stripped)\n",
        "        return unique\n",
        "\n",
        "    initial_candidates = await run_with_spinner(\n",
        "        gather_all,\n",
        "        message=\"Waiting to finish generating candidate prompts...\"\n",
        "    )\n",
        "\n",
        "    if not initial_candidates:\n",
        "        rich_display(\"No initial candidates were generated.\", \"warning\")\n",
        "    else:\n",
        "        for i, candidate in enumerate(initial_candidates):\n",
        "            rich_display(f\"Candidate {i + 1} >> {candidate}\", \"evaluating\")\n",
        "\n",
        "    return initial_candidates\n",
        "\n",
        "\n",
        "def select_candidate(P: list[str], S: np.ndarray) -> tuple[list[int], list[float]]:\n",
        "    \"\"\"P=prompt candidate pool; S=score matrix on Dpareto\"\"\"\n",
        "\n",
        "    num_tasks = S.shape[1]\n",
        "    num_candidates = S.shape[0]\n",
        "\n",
        "    rich_display(f\"There are {num_candidates} candidates in the pool.\", \"info\")\n",
        "\n",
        "    if len(P) == 1:\n",
        "        return [0], [1.0]\n",
        "\n",
        "    # Build instance-wise Pareto sets\n",
        "    s_star = np.max(S, axis=0)  # Best score for each task\n",
        "    P_star = [set() for _ in range(num_tasks)]\n",
        "\n",
        "    for i in range(num_tasks):\n",
        "        for j in range(num_candidates):\n",
        "            if S[j, i] == s_star[i]:\n",
        "                P_star[i].add(j)    # Store the index of the candidate\n",
        "\n",
        "    # Unique candidates in union for P_star[i]\n",
        "    C = set()\n",
        "    for p_set in P_star:\n",
        "        C.update(p_set)\n",
        "\n",
        "    # Filter dominated candidates\n",
        "    D = set()\n",
        "    C_list = list(C)\n",
        "\n",
        "    while True:\n",
        "        dominated_found = False\n",
        "        for idx1 in range(len(C_list)):\n",
        "            if C_list[idx1] in D:\n",
        "                continue\n",
        "            is_dominated = False\n",
        "            for idx2 in range(len(C_list)):\n",
        "                if idx1 == idx2 or C_list[idx2] in D:\n",
        "                    continue\n",
        "                # Check if C_list[idx1] is dominated by C_list[idx2]\n",
        "                # Dominated: for all tasks, S[idx1][i] <= S[idx2][i] and strict for at least one\n",
        "                all_leq = all(S[C_list[idx1]][i] <= S[C_list[idx2]][i] for i in range(num_tasks))\n",
        "                any_lt = any(S[C_list[idx1]][i] < S[C_list[idx2]][i] for i in range(num_tasks))\n",
        "                if all_leq and any_lt:\n",
        "                    is_dominated = True\n",
        "                    break\n",
        "            if is_dominated:\n",
        "                D.add(C_list[idx1])\n",
        "                dominated_found = True\n",
        "        if not dominated_found:\n",
        "            break\n",
        "\n",
        "    # Remove D from each P_star[i] to get hat{P_star}[i]\n",
        "    hat_P_star = [p_set - D for p_set in P_star]\n",
        "    f = Counter()\n",
        "    for p_set in hat_P_star:\n",
        "        for k in p_set:\n",
        "            f[k] += 1\n",
        "\n",
        "    # Sample from hat_C (unique in hat_P_star) with prob proportional to f[k]\n",
        "    hat_C = list(set(k for p_set in hat_P_star for k in p_set))\n",
        "    if not hat_C:\n",
        "        raise ValueError(\"No candidates available after filtering dominated candidates.\")\n",
        "    probs = [f[k] for k in hat_C]\n",
        "    total = sum(probs)\n",
        "    probs = [p / total for p in probs]\n",
        "\n",
        "    rich_display(f\"Selected {len(hat_C)} candidates from the pool.\", \"info\")\n",
        "    rich_display(f\"Candidate IDs: {hat_C}\", \"info\")\n",
        "    rich_display(f\"Selection probabilities: {probs}\", \"info\")\n",
        "    return hat_C, probs\n",
        "\n",
        "async def GEPA(\n",
        "    Dpareto: list[dict[str, str]],\n",
        "    Dfeedback: list[dict[str, str]],\n",
        "    meta_prompt: str,\n",
        "    merging_prompt: str,\n",
        "    eval_metric: callable,\n",
        "    eval_feedback: callable,\n",
        "    initial_candidates: list[str],\n",
        "    budget: int = 50,\n",
        "    mini_batch_size: int = 2\n",
        ") -> None:\n",
        "    \"\"\"Returns the optimal prompt for a given task.\"\"\"\n",
        "    print(\"-\" * 50)\n",
        "    rich_display(\"Starting GEPA...\", \"info\")\n",
        "    rich_display(f\"Target model: {TARGET_MODEL}\\n\"\n",
        "                 f\"Reflection model: {REFLECTION_MODEL}\\n\"\n",
        "                 f\"No. of Pareto samples: {len(Dpareto)}\\n\"\n",
        "                 f\"No. of feedback samples: {len(Dfeedback)}\\n\"\n",
        "                 f\"Mini-batch size: {mini_batch_size}\\n\"\n",
        "                 f\"Budget: {budget}\", \"info\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    candidate_prompts = []\n",
        "    for ith, candidate_prompt_txt in enumerate(initial_candidates):\n",
        "        candidate_prompts.append({\"id\": ith, \"parent_id\": -1, \"prompt\": candidate_prompt_txt,\n",
        "                                  \"scores\": [0] * len(Dpareto), \"mean_score\": 0.0})\n",
        "\n",
        "    # Initialize S-matrix\n",
        "    S = np.zeros((len(candidate_prompts), len(Dpareto)), dtype=float)\n",
        "\n",
        "    rich_display(\"Evaluating initial candidates...\", \"info\")\n",
        "\n",
        "    # Concurrent evaluation of initial candidates\n",
        "    async def _eval_candidate(j: int, candidate: dict[str, Any]) -> int:\n",
        "        candidate_prompt = candidate[\"prompt\"]\n",
        "        rich_display(f\"Evaluating candidate (id={candidate['id']}) >> {candidate_prompt}\", \"evaluating\")\n",
        "        local_budget_used = 0\n",
        "        for h, xi_gt in enumerate(Dpareto):\n",
        "            xi, gt = xi_gt.get(\"question\", \"\"), xi_gt.get(\"answer\", \"\")\n",
        "            if not xi or not gt:\n",
        "                rich_display(\"Sample is missing question or answer.\", \"error\")\n",
        "                raise ValueError(\"Sample is missing question or answer.\")\n",
        "\n",
        "            contents = f\"{candidate_prompt}\\n\\nQuery: {xi}\\n\"\n",
        "            pred = await extract_response_from_target(contents=contents, format=Response)\n",
        "\n",
        "            score = eval_metric(pred, gt)\n",
        "            S[j, h] = score\n",
        "            candidate[\"scores\"][h] = score\n",
        "\n",
        "            if score == 1:\n",
        "                rich_display(f\"Candidate (id={candidate['id']}) got sample {h + 1} right! [prediction={pred}; ground truth={gt}]\", \"success\")\n",
        "            else:\n",
        "                rich_display(f\"Candidate (id={candidate['id']}) got sample {h + 1} wrong. [prediction={pred}; ground truth={gt}]\", \"error\")\n",
        "            local_budget_used += 1\n",
        "        else:\n",
        "            candidate[\"mean_score\"] = float(np.mean(candidate[\"scores\"]))\n",
        "            rich_display(f\"Candidate (id={candidate['id']}) mean score: {candidate['mean_score']}\", \"scoring\")\n",
        "\n",
        "        return local_budget_used\n",
        "\n",
        "    # Launch all candidate evaluations concurrently\n",
        "    budget_used_list = await asyncio.gather(\n",
        "        *[ _eval_candidate(j, candidate) for j, candidate in enumerate(candidate_prompts) ]\n",
        "    )\n",
        "\n",
        "    budget -= sum(budget_used_list)\n",
        "    rich_display(f\"Budget status: {budget}\", \"info\")\n",
        "\n",
        "    rich_display(f\"Starting GEPA optimization loop with budget = {budget} left...\", \"info\")\n",
        "    while budget > 0:\n",
        "        # Sample a mini-batch of feedback samples\n",
        "        mini_batch_indices = random.sample(range(len(Dfeedback)), mini_batch_size)\n",
        "        Dminibatch = [Dfeedback[i] for i in mini_batch_indices]\n",
        "\n",
        "        # Exploration (1-P) vs. exploitation (P)\n",
        "        # P probability to pick the best\n",
        "        # 1-P probability to pick random sample\n",
        "        global P\n",
        "        global Q\n",
        "\n",
        "        if random.random() <= P:\n",
        "            # Choose the best based on strategy\n",
        "            selected_candidate_ids, probs = select_candidate(candidate_prompts, S)\n",
        "            if len(selected_candidate_ids) > 1:\n",
        "                # 1-Q probability to perform candidate prompt-merge\n",
        "                # Q probability to choose one based on probs\n",
        "                if random.random() <= Q:\n",
        "                    rich_display(\"[Exploit-Normal] Selecting one candidate based on probabilities...\", \"comparing\")\n",
        "                    selected_candidate_id = random.choices(selected_candidate_ids, weights=probs, k=1)[0]\n",
        "                    selected_candidate = candidate_prompts[selected_candidate_id]\n",
        "                else:\n",
        "                    # Perform candidate prompt-merge\n",
        "                    rich_display(\"[Exploit-Merge] Performing candidate prompt-merge on selected candidates...\", \"comparing\")\n",
        "                    candidate_parents = ()\n",
        "                    for_merging_candidates = \"\"\n",
        "                    for selected_candidate_id in selected_candidate_ids:\n",
        "                        candidate_p = candidate_prompts[selected_candidate_id]\n",
        "                        candidate_parents += (candidate_p[\"id\"],)\n",
        "                        for_merging_candidates += f\"Candidate {candidate_p['id']} >> {candidate_p['prompt']}\\n\"\n",
        "\n",
        "                    merged_prompt = await extract_merged_prompt(\n",
        "                        for_merging_candidates=for_merging_candidates,\n",
        "                        merging_prompt=merging_prompt,\n",
        "                        format=ListCandidatePrompts\n",
        "                    )\n",
        "                    selected_candidate = {\n",
        "                        \"id\": len(candidate_prompts),\n",
        "                        \"parent_id\": candidate_parents,\n",
        "                        \"prompt\": merged_prompt,\n",
        "                        \"scores\": [0] * len(Dpareto),\n",
        "                        \"mean_score\": 0.0\n",
        "                    }\n",
        "                    candidate_prompts.append(selected_candidate)\n",
        "            else:\n",
        "                rich_display(\"[Exploit-Max] Selecting the best candidate based on probabilities...\", \"comparing\")\n",
        "                selected_candidate = candidate_prompts[selected_candidate_ids[0]]\n",
        "        else:\n",
        "            # Choose a random candidate from the pool\n",
        "            rich_display(\"[Explore] Selecting a random candidate from the pool...\", \"comparing\")\n",
        "            selected_candidate_id = random.choice(range(len(candidate_prompts)))\n",
        "            selected_candidate = candidate_prompts[selected_candidate_id]\n",
        "\n",
        "        rich_display(f\"Selected candidate (id={selected_candidate['id']}) >> {selected_candidate['prompt']}\", \"evaluating\")\n",
        "\n",
        "        # Generate feedback for the selected candidate\n",
        "        overall_feedback = \"\"\n",
        "        selected_candidate_feedback = []\n",
        "        S_feedback = []\n",
        "        # --- Concurrent evaluation of selected_candidate on Dminibatch ---\n",
        "        async def _eval_minibatch_sample(xi_gt):\n",
        "            xi, gt = xi_gt.get(\"question\", \"\"), xi_gt.get(\"answer\", \"\")\n",
        "            if not xi or not gt:\n",
        "                raise ValueError(\"Sample is missing question or answer.\")\n",
        "            contents = f\"{selected_candidate['prompt']}\\n\\nQuery: {xi}\\n\"\n",
        "            pred = await extract_response_from_target(contents=contents, format=Response)\n",
        "            score = eval_metric(pred, gt)\n",
        "            feedback = eval_feedback(score)\n",
        "            return {\n",
        "                \"query\": xi,\n",
        "                \"prediction\": pred,\n",
        "                \"ground_truth\": gt,\n",
        "                \"score\": score,\n",
        "                \"feedback\": feedback\n",
        "            }\n",
        "\n",
        "        rich_display(f\"Evaluating selected candidate (id={selected_candidate['id']}) on Dminibatch...\", \"info\")\n",
        "        minibatch_tasks = [asyncio.create_task(_eval_minibatch_sample(xi_gt)) for xi_gt in Dminibatch]\n",
        "        selected_candidate_feedback = await asyncio.gather(*minibatch_tasks)\n",
        "\n",
        "        # Budget consumed equals number of samples evaluated\n",
        "        budget -= len(selected_candidate_feedback)\n",
        "\n",
        "        # Logging per-sample results\n",
        "        for i, fobj in enumerate(selected_candidate_feedback):\n",
        "            if fobj[\"score\"] == 1:\n",
        "                rich_display(f\"Candidate (id={selected_candidate['id']}) got sample {i + 1} right! [prediction={fobj['prediction']}; ground truth={fobj['ground_truth']}]\", \"success\")\n",
        "            else:\n",
        "                rich_display(f\"Candidate (id={selected_candidate['id']}) got sample {i + 1} wrong. [prediction={fobj['prediction']}; ground truth={fobj['ground_truth']}]\", \"error\")\n",
        "\n",
        "        S_feedback = [fobj[\"score\"] for fobj in selected_candidate_feedback]\n",
        "        mean_feedback_score = float(np.mean(S_feedback))\n",
        "        rich_display(f\"Candidate (id={selected_candidate['id']}) mean feedback score: {mean_feedback_score}\", \"scoring\")\n",
        "\n",
        "        overall_feedback = \"\\n\".join(\n",
        "            f\"Query: {f['query']}\\n\"\n",
        "            f\"Assistant's Response: {f['prediction']}\\n\"\n",
        "            f\"Correct Response: {f['ground_truth']}\\n\"\n",
        "            f\"Score: {f['score']}\\n\"\n",
        "            f\"Feedback: {f['feedback']}\\n\"\n",
        "            for f in selected_candidate_feedback\n",
        "        )\n",
        "        rich_display(f\"Overall feedback for candidate (id={selected_candidate['id']}) generated.\", \"success\")\n",
        "\n",
        "        # Reflection / Mutation\n",
        "        rich_display(\"Evolving selected prompt via reflection...\", \"info\")\n",
        "        meta_contents = meta_prompt.format(\n",
        "            candidate=selected_candidate['prompt'],\n",
        "            inputs_outputs_feedback=overall_feedback\n",
        "        )\n",
        "        reflection_candidates = await extract_response_from_reflection(contents=meta_contents, format=ListCandidatePrompts)\n",
        "        if not reflection_candidates:\n",
        "            rich_display(\"Reflection model returned no candidates. Skipping mutation...\", \"warning\")\n",
        "            continue\n",
        "        new_candidate_prompt = reflection_candidates[0]\n",
        "        rich_display(f\"New candidate prompt generated >> {new_candidate_prompt}\", \"evaluating\")\n",
        "\n",
        "        # Concurrent evaluation of new candidate on Dminibatch\n",
        "        async def _eval_new_minibatch_sample(xi_gt):\n",
        "            xi, gt = xi_gt.get(\"question\", \"\"), xi_gt.get(\"answer\", \"\")\n",
        "            if not xi or not gt:\n",
        "                raise ValueError(\"Sample is missing question or answer.\")\n",
        "            contents = f\"{new_candidate_prompt}\\n\\nQuery: {xi}\\n\"\n",
        "            pred = await extract_response_from_target(contents=contents, format=Response)\n",
        "            score = eval_metric(pred, gt)\n",
        "            return score\n",
        "\n",
        "        rich_display(\"Evaluating new candidate on Dminibatch...\", \"info\")\n",
        "        new_minibatch_tasks = [asyncio.create_task(_eval_new_minibatch_sample(xi_gt)) for xi_gt in Dminibatch]\n",
        "        new_candidate_scores = await asyncio.gather(*new_minibatch_tasks)\n",
        "        budget -= len(new_candidate_scores)\n",
        "\n",
        "        for i, score in enumerate(new_candidate_scores):\n",
        "            if score == 1:\n",
        "                rich_display(f\"New candidate got sample {i + 1} right! [prediction={score}; ground truth={Dminibatch[i]['answer']}]\", \"success\")\n",
        "            else:\n",
        "                rich_display(f\"New candidate got sample {i + 1} wrong. [prediction={score}; ground truth={Dminibatch[i]['answer']}]\", \"error\")\n",
        "\n",
        "        mean_new_candidate_score = float(np.mean(new_candidate_scores))\n",
        "        rich_display(f\"New candidate mean score on Dminibatch: {mean_new_candidate_score}\", \"scoring\")\n",
        "\n",
        "        if mean_new_candidate_score >= mean_feedback_score:\n",
        "            # Concurrent evaluation on full Dpareto\n",
        "            rich_display(\"Score improvement detected. Evaluating on Dpareto...\", \"info\")\n",
        "\n",
        "            async def _eval_pareto_sample(xi_gt):\n",
        "                xi, gt = xi_gt.get(\"question\", \"\"), xi_gt.get(\"answer\", \"\")\n",
        "                if not xi or not gt:\n",
        "                    raise ValueError(\"Sample is missing question or answer.\")\n",
        "                contents = f\"{new_candidate_prompt}\\n\\nQuery: {xi}\\n\"\n",
        "                pred = await extract_response_from_target(contents=contents, format=Response)\n",
        "                score = eval_metric(pred, gt)\n",
        "                return score, pred\n",
        "\n",
        "            pareto_tasks = [asyncio.create_task(_eval_pareto_sample(xi_gt)) for xi_gt in Dpareto]\n",
        "            pareto_results = await asyncio.gather(*pareto_tasks)\n",
        "            budget -= len(pareto_results)\n",
        "\n",
        "            new_candidate_scores_Dpareto = []\n",
        "            for j, (score, pred) in enumerate(pareto_results):\n",
        "                new_candidate_scores_Dpareto.append(score)\n",
        "                if score == 1:\n",
        "                    rich_display(f\"New candidate prompt got sample {j + 1} right! [prediction={pred}; ground truth={Dpareto[j]['answer']}]\", \"success\")\n",
        "                else:\n",
        "                    rich_display(f\"New candidate prompt got sample {j + 1} wrong. [prediction={pred}; ground truth={Dpareto[j]['answer']}]\", \"error\")\n",
        "\n",
        "            # Add new candidate prompt to pool\n",
        "            new_candidate_dict = {\n",
        "                \"id\": len(candidate_prompts),\n",
        "                \"parent_id\": selected_candidate['id'],\n",
        "                \"prompt\": new_candidate_prompt,\n",
        "                \"scores\": new_candidate_scores_Dpareto,\n",
        "                \"mean_score\": float(np.mean(new_candidate_scores_Dpareto))\n",
        "            }\n",
        "            candidate_prompts.append(new_candidate_dict)\n",
        "\n",
        "            # Update S-matrix\n",
        "            S = np.vstack((S, new_candidate_scores_Dpareto))\n",
        "            rich_display(f\"New candidate prompt (id={new_candidate_dict['id']}) added to candidate prompt pool.\", \"success\")\n",
        "            rich_display(f\"New candidate prompt info :: 'parent_id': {new_candidate_dict['parent_id']}, \"\n",
        "                         f\"'mean_score': {new_candidate_dict['mean_score']}\", \"info\")\n",
        "        else:\n",
        "            rich_display(\"New candidate prompt did not improve over the selected candidate prompt. Skipping addition to candidate prompt pool...\", \"warning\")\n",
        "\n",
        "    # Display the best candidate prompt\n",
        "    print(\"-\" * 50)\n",
        "    rich_display(\"GEPA optimization completed.\", \"success\")\n",
        "    best_candidate_id = np.argmax([c[\"mean_score\"] for c in candidate_prompts])\n",
        "    best_candidate = candidate_prompts[best_candidate_id]\n",
        "    rich_display(f\"Best candidate prompt (id={best_candidate['id']}) >> {best_candidate['prompt']}\", \"success\")\n",
        "    rich_display(f\"Best candidate prompt info :: 'parent_id': {best_candidate['parent_id']}, \"\n",
        "                 f\"'mean_score': {best_candidate['mean_score']}\", \"info\")\n",
        "\n",
        "\n",
        "# Wrap GEPA inside a spinner\n",
        "async def run_gepa(\n",
        "    Dpareto: list[dict[str, str]],\n",
        "    Dfeedback: list[dict[str, str]],\n",
        "    meta_prompt: str,\n",
        "    merging_prompt: str,\n",
        "    eval_metric: callable,\n",
        "    eval_feedback: callable,\n",
        "    initial_candidates: list[str],\n",
        "    budget: int = 50,\n",
        "    mini_batch_size: int =2\n",
        ") -> None:\n",
        "    async def _gepa():\n",
        "        await GEPA(\n",
        "            Dpareto=Dpareto,\n",
        "            Dfeedback=Dfeedback,\n",
        "            meta_prompt=meta_prompt,\n",
        "            merging_prompt=merging_prompt,\n",
        "            eval_metric=eval_metric,\n",
        "            eval_feedback=eval_feedback,\n",
        "            initial_candidates=initial_candidates,\n",
        "            budget=budget,\n",
        "            mini_batch_size=mini_batch_size\n",
        "        )\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    await run_with_spinner(\n",
        "        task=_gepa,\n",
        "        message=\"Waiting to finish GEPA optimization...\"\n",
        "    )\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    rich_display(f\"GEPA optimization completed in {elapsed_time:.2f} seconds.\", \"info\")\n",
        "\n",
        "\n",
        "async def main():\n",
        "    global META_PROMPT, MERGING_PROMPT, BUDGET, MINI_BATCH_SIZE, NUM_INITIAL_CANDIDATE_PROMPTS\n",
        "\n",
        "    with open(\"Dpareto.json\", \"r\") as fPareto:\n",
        "        Dpareto = json.load(fPareto)\n",
        "\n",
        "    with open(\"Dfeedback.json\", \"r\") as fFeedback:\n",
        "        Dfeedback = json.load(fFeedback)\n",
        "\n",
        "    initial_candidates = await generate_initial_candidates_from_seed(\n",
        "        seed_prompt=SEED_PROMPT,\n",
        "        initial_candidates_generation_prompt=INITIAL_CANDIDATES_GENERATION_PROMPT,\n",
        "        Dpareto=Dpareto,\n",
        "        num_new_prompts=NUM_INITIAL_CANDIDATE_PROMPTS\n",
        "    )\n",
        "\n",
        "    await run_gepa(\n",
        "        Dpareto=Dpareto,\n",
        "        Dfeedback=Dfeedback,\n",
        "        meta_prompt=META_PROMPT,\n",
        "        merging_prompt=MERGING_PROMPT,\n",
        "        eval_metric=eval_metric,\n",
        "        eval_feedback=eval_feedback,\n",
        "        initial_candidates=initial_candidates,\n",
        "        budget=BUDGET,\n",
        "        mini_batch_size=MINI_BATCH_SIZE\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ]
    }
  ]
}